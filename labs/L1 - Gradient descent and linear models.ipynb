{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 - Градиентый спуск и линейные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Линейные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть есть обучающая выборка $\\{x_i\\}_{i=1}^{\\mathcal{l}} \\subset \\mathbb{R}^{n}$, при этом каждому объекту в соответсвие поставлена метка класса $y_{i} \\in \\{-1, +1\\}$. Мы предполагаем, что в пространтсве $\\mathbb{R}^{n}$ существует гиперплоскость, которая относительно некоторой метрики \"хорошо\" разделяет объекты на два класса. При этом гиперплоскость задается параметрически:\n",
    "\n",
    "<center>\n",
    "$wx + b = 0$\n",
    "</center>\n",
    "\n",
    "Объект $x$ имеет метку $y = +1$, если $wx + b \\geq 0$ и $y = -1$ в ином случае. Вектор $w$ является нормалью к гиперплоскости, указывающий с какой стороны находятся объекты класса $y = +1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поиск модели ограничен до одного семейства, заданного параметрически. Обучение в таком случае сводится к задаче оптимизации эмпирического риска\n",
    "\n",
    "<center>\n",
    "$\\arg \\min_{\\theta} Q(\\theta) = \\arg \\min_{\\theta} \\frac{1}{l}\\sum_{i=1}^{\\mathcal{l}} \\mathcal{L}(a(x_i|\\theta), y_i)$, где\n",
    "</center>\n",
    "\n",
    "* $a(x|\\theta)$ - алгоритм из некотрого семейства, заданный параметром $\\theta$\n",
    "* $\\theta$ - вектор пространства параметров\n",
    "* $\\mathcal{L}$ - функция потерь, которая показывает на сколько точно предсказание\n",
    "\n",
    "Очевидно, что качество предсказания зависит от выбранной модели. Но также оно зависит и от выбора функции потерь $\\mathcal{L}$, которая существенно влияет на процесс обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Функция потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В литературе можно встретить такое понятие, как отступ\n",
    "<center>$ M(x, y) = y\\cdot(wx + b)$,</center>\n",
    "его можно трактовать, как уровень удаление от гиперплоскости в сторону своего класса. Это позволит нам кратко записывать функции потерь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее естественной функцией потерь для задачи классификации является относительное количество неправильных классификаций, то есть\n",
    "<center>$ \\mathcal{L}(y_{pred}, y_{true}) = [y_{pred} \\neq y_{true}] = [M(x, y_{true}) < 0]$</center>\n",
    "\n",
    "Решение такой задачи является очень трудоемким, поэтому на практике производят оптимизацию реклаксированной ошибки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К примеру **квадратичная ошибка**\n",
    "\n",
    "<center>$ Q(w) = \\frac{1}{\\mathcal{l}} \\sum_{i=1}^{\\mathcal{l}}((wx_i+b) - y_i)^{2}$</center>\n",
    "\n",
    "Она многим хороша, к примеру, в задачи оптимизации все сводится к выпуклому функционалу с локальным минимумом. Если представить, что признаки объекта $x_i$ записаны в матрицу $X$ построчно, а все метки записаны в вектор-столбец $Y$, то задача выглядит\n",
    "\n",
    "<center>\n",
    "$\\arg\\min_{w}||Xw - Y ||_{2}$,\n",
    "</center>\n",
    "\n",
    "и имеет аналитическое решение\n",
    "\n",
    "<center>\n",
    "$w = (X^TX)^{-1}X^TY$.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "\n",
    "1. Сгенерируйте на плоскости 2 облака точек. Они должны слегка пересекаться, а точки внутри распределены нормально.\n",
    "2. Обучите линейную модель, разделяющую два облака точек, использую формулу выше.\n",
    "3. Изобразите облака с помощью библиотеки matplotlib, воспользуйтесь функцией scatter, для удобства точки можно сделать прозрачными.\n",
    "4. Постройте полученнную разделяющую прямую.\n",
    "5. Оцените сложность алгоритма обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще популярна следующая релаксация\n",
    "<center>$Q(w) = \\frac{1}{\\mathcal{l}} \\sum_{i=1}^{\\mathcal{l}} max(0, 1 - y_i\\cdot(wx_i + b))$,</center>\n",
    "если хотите узнать об этом более подробно, то вам стоит почитать про svm (support vector machine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая функция же обладает вероятностным смыслом\n",
    "\n",
    "<center>$ Q(w) = \\frac{1}{\\mathcal{l}} \\sum_{i=1}^{\\mathcal{l}} \\ln(1 + \\exp(-y_i\\cdot(wx_i + b)))$</center>\n",
    "В частности данный функционал приводит нас к оптимальному байесовскому классификатору при некоторых допущениях о распределении признаков. Но это совершенно отдельная история."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "\n",
    "1. Пусть $\\mathbb{P}\\{y=1|x\\} = \\sigma(wx+b)$, где $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$. Покажите, что задача\n",
    "<center>$ \\arg\\min_{w, b} \\sum_{x, y} \\ln(1 + \\exp(-y(wx + b)))$</center>\n",
    "есть ничто иное, как максимазиция правдоподобия.\n",
    "2. Отобразите все функционалы качества в осях $M \\times Q$ для одного элемента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для задачи оптимизации не всегда существует аналитическое решение, либо оно может быть очень сложным. В таком случае используют численные методы. Да, речь идет именно о градиентном спуске. Это итеративный алгоритм, который устроен следующим образом. Пусть есть $Q(x)$, которую необходимо оптимизировать и она дифференцируема. Тогда задачу\n",
    "\n",
    "<center>$ \\arg\\min_{x} Q(x)$</center>\n",
    "\n",
    "можно решить следующим образом\n",
    "\n",
    "<center>$ x^{k+1} = x^{k} - \\lambda \\cdot \\triangledown Q(x)$,</center>\n",
    "\n",
    "где $\\lambda$ - некоторый шаг градиентного спуска, а $k$ - номер этого шага.\n",
    "\n",
    "От выбора правильного $\\lambda$ сильно зависит процесс обучения. Если взять слишком большое значение, то алгоритм может не сойтись. Если слишком малое, то обучение будет длиться долго. Также существует распространенный прием, применяемый часто при обучении нейросетей, уменьшать значение $\\lambda$ в соответствии с некоторым расписанием."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "1. Предложите какую-нибудь квадратичную функцию с глобальным минимумом.\n",
    "2. Найдите минимум методом градиентного спуска.\n",
    "3. Отобразите на плоскости линии уровней функции, которую вы оптимизируете.\n",
    "4. Покажите, какую траекторию проходит алгоритм градиентного спуска.\n",
    "5. Как вы выбрали значение $\\lambda$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют функции, которые плохо даются градиентному спуску. К примеру, функция Розенброка\n",
    "\n",
    "<center>$f(x, y) = (1-x)^2 + 100(y-x^2)^2$.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "1. Проделайте все то же самое для функции Розенброка.\n",
    "2. Какую проблему вы наблюдаете?\n",
    "3. Как ее можно решить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют различные модификации алгоритма градиентного спуска. К примеру, метод наискорейшего спуска, где значение $\\lambda$ зависит от шага\n",
    "\n",
    "<center>$\\lambda^{k} = \\arg\\min_{\\lambda}Q(x_k - \\lambda\\triangledown Q(x_k))$.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "1. Снова разделите облака точек, только теперь оптимизируйте квадратичную ошибку метода градиентного спуска.\n",
    "2. Отобразите полученную прямую и облака точек.\n",
    "3. Сравните ответ с точным решением.\n",
    "4. Попробуйте метод наискорейшего спуска.\n",
    "5. Постройте график в осях (номер шага и значение $Q$).\n",
    "6. Сравните скорость сходимости обычного и наискорейшего спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И еще немного о проблемах градиентного спуска. Если у нас есть какие-то признаки, которые встречаются достаточно редко, то соответствующий столбец будет разряженным.\n",
    "\n",
    "**Задание**\n",
    "В чем заключается проблема?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также нужно понимать, что градиентный спуск может попасть в \"ловушку\" локального минимума. Обычно это актуально для нейросетей. Самый простой способо решить эту проблема - сдедать несколько запусков алгоритма или иметь какой-то инсайд, из какой точки стоит начинать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Стохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда количество данных может быть так велико, что даже градиентный спуск начинает работать медленно. Или же данные просто поступают к нам большим потоком, а параметры модели постепенно меняются. Тогда на сцену выходит метод стохастического градиента.\n",
    "\n",
    "Идея пределельно проста. Можно делать шаг спуска, вычисляя ошибку и градиент не для всех элементов выборки, а для какого-то небольшого количества или даже для одного объекта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "\n",
    "1. Скачайте данные mnist c [Kaggle](https://www.kaggle.com/c/digit-recognizer).\n",
    "2. Обучите линейный классификатор 0 и 1, используйте логистическую функцию потерь.\n",
    "3. Проверьте качество классификации на отложенной выборке.\n",
    "<center>$ \\mathcal{L}(y_{pred}, y_{true}) = [y_{pred} \\neq y_{true}]$ </center>\n",
    "4. Как влияет размер батча на скорость и качество обучения?\n",
    "5. Отобразите графики, которые доказывает ваши слова (оси придумайте сами).\n",
    "6. Сколько проходов по данным вы делаете? Почему?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У стохастического градиентного спуска также есть много всяких усовершествований, которые часто используются в реальной практике при обучении нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кстати, текущее значение $Q$ можно вычислять с помощью экспоненциального сглаживания.\n",
    "<center>$Q^{k+1} = \\gamma Q^k + (1 - \\gamma) Q(x_{k+1})$, </center>\n",
    "\n",
    "где $Q(x_{k+1})$ вычисляется для обрабатываемого батча.\n",
    "\n",
    "**Задание**\n",
    "1. Как зависит график от $\\gamma$?\n",
    "2. Каким способом лучше вычислять $Q$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сохранение импульса**\n",
    "\n",
    "Сохранения импульса позволяет избежать нам осциляции вдоль оси, по которой функция изменяется сильнее. Он заключается в том, что текущий градиентный шаг вычисляется на основе учета предыдущих шагов\n",
    "<center> $x^{k+1} = x^{k} - s^{k}$,</center> где $s^k = \\gamma s^{k-1} + \\lambda\\triangledown Q(x^k)$, при этом\n",
    " * $0 <\\gamma < 1$ - коэффициент учета предыдущего импульса\n",
    " * $s^{-1} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "\n",
    "1. Найдите минимум $Q(x, y) = 10x^2 + y^2$ c помощью обычного метода.\n",
    "2. Воспользуйтесь методом сохранения импульса\n",
    "3. Отобразите и сравните треки.\n",
    "4. На основе чего вы выбрали $\\gamma$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ускоренный градиент Нестерова**\n",
    "\n",
    "И логическое развитие этого подхода приводит к методу ускоренного градиента Нестерова. Шаг спуска вычисляется немного иначе\n",
    "<center>$s^k = \\gamma s^{k-1} + \\lambda\\triangledown Q(x^k - s^{k-1})$,</center>\n",
    "то есть мы вычисляем градиент фукнции примерно в той точке, куда \"занесет\" нас накопленный импульс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "\n",
    "1. Сравните этот метод и предыдущий на функции Розенброка.\n",
    "2. Отобразите и сравните треки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adagrad (2011)**\n",
    "\n",
    "Адаптивный градиент подразумевает вычисление $\\lambda$ для каждой размерности входного вектора $x$. Неформально говоря, для разряженных признаков он делает больший шаг, а для обычных шаг поменьше.\n",
    "<center> $x_{i}^{k + 1} = x_{i}^{k} - \\frac{\\lambda}{\\sqrt{G_{i, i}^k } + \\varepsilon} \\cdot \\frac{\\partial Q}{\\partial x_i}(x^k)$, где \n",
    "</center>\n",
    "\n",
    "\n",
    "* $G^{k} = \\sum_{t=1}^{k}g_t g_t^{T}$, где $g_t = \\triangledown Q(x^t)$.\n",
    "* $\\varepsilon$ - небольшая добавка, чтобы избежать деление на ноль.\n",
    "\n",
    "Как ни странно это улучшает сходимость процесса обучение, к примеру, при работе нейросетей с текстами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "\n",
    "1. Обучите модель этим способом для mnist.\n",
    "2. Сравните сходимость с обычным стохастическим градиентным спуском (графики)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RMSprop**\n",
    "\n",
    "Чтобы избежать постоянный рост знаменателя при $\\lambda$ можно воспользоваться следующим вариантом шага. Давайте будем подсчитывать матрицу $G^k$ только для какого-то небольшого количества последних шагов, это можно сделать к примеру с помощью экспоненциального сглаживания\n",
    "\n",
    "<center>$G^{k+1} = \\gamma G^{k} + (1 - \\gamma)g_{k+1}g_{k+1}^{T}$, где</center>\n",
    "$0< \\gamma < 1$ - чем больше значение, тем большее количество последних элементов мы учитываем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adadelta (2012)**\n",
    "\n",
    "**Задание**\n",
    "1. Изучите метод самостоятельно и кратко опишите.\n",
    "2. Как вы можете его объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adam (2015)**\n",
    "\n",
    "**Задание**\n",
    "1. Попробуйте скомбинировать метод сохранения импульса и RMSprop.\n",
    "2. Получили ли вы какое-то улучшение?\n",
    "3. Почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "1. Предложите некоторую функцию, которая наглядно показываеn отличие в работе всех предложенных методов.\n",
    "2. Сделайте анимацию, которая пошагово отрисовывает треки все спусков."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
